---
title: "Predicting Next Word Using Katz Back-Off: Preprocessing & EDA"
author: "Michael Szczepaniak"
date: "June 2016"
output: html_document
url: http://rpubs.com/mszczepaniak/predictnextkbo
---
## Background  
World wide mobile internet usage is projected to continue its rapid growth over the next few years.  According to a [Statista Fact Sheet](http://www.statista.com/statistics/284202/mobile-phone-internet-user-penetration-worldwide/), the percentage of mobile phone users accessing the internet will rise to 63.4% in 2019 up from 48.8% in 2014.  This increased ownship has resulted in more people spending increasing amounts of time on mobile devices for email, social networking, banking and other activities. Because typing on these devices is an awkward and tedious task, smart keyboard applications based on predictive text analytics have emerged to make typing easier.  

## Objective
The goal of this project is to develop a prototype predictive web application that suggests the next word in a text based message based on what has been typed in by the user. For example, a user may type *I love Italian* and the the application might suggest: *food*, *shoes*, or *opera*.  A Katz Back-Off (KBO) Trigram  Language Model (LM) was selected as the algorithm to make these predictions.  A detailed description of how this algorithm works can be found here.  The KBO Trigram was chosen for 2 primary reasons:

1. **Size -** One of the simpliest LMs considered was the [A Stupid Back-Off (SBO) model](http://www.aclweb.org/anthology/D07-1090.pdf), but this model tends to do well with large web-scale n-gram tables.  The authors of this algorithm built tarabyte-sized language models.  Since this app was deployed on a 1Gb shinyapp.io instance, I didn't think I'd have enough space for the n-grams tables needed to do a decent job with predictions.
2. **Accuracy on small corpus -** The SBO does not do any estimation on unobserved n-grams, but instead backs off to the nearest matched n-gram until it reaches the unigram.  The KBO model incorporates a form of smoothing in order to estimate probabilities of unobserved n-grams which appears to be more accurate when using a limited number of n-grams (trigrams in this case).

In addition to the three reasons listed above, because the SBO is much simplier to implement, I felt the project would be more meaningful and challenging to work on something a bit more complex like the KBO.

## Acquiring and Cleaning the Data
### Acquiring and Reading the Data
The data was originally downloaded from [this link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and stored locally. If this link is no longer available, the data can obtained from [my dropbox](https://www.dropbox.com/s/uk7d4gp1c7bgmxc/Coursera-SwiftKey.zip?raw=1). The zip file was download to a directory called **data** in a local project and unzipped there.  The the unzipped data contained four subdirectories: **de_DE** (German), **en_US** (US english), **fi_FI** (Finnish), and **ru_RU** (Russian). This project focuses on the English corpora residing in the **en_US** folder. This folder contains three files named **en_US.blogs.txt**, **en_US.news.txt**, and **en_US.twitter.txt**.

With the data residing locally, the following code was used to read each file into a character array.

```{r message=FALSE, warning=FALSE, results='hide'}
# install packages if needed
list.of.packages <- c("dplyr", "readr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages) > 0) install.packages(new.packages)
# load libraries
libs <- c("dplyr", "readr")
lapply(libs, require, character.only=TRUE)  # load libs
options(stringsAsFactors = FALSE)  # strings are what we are operating on...
dataDir <- '../data/en_US' # data dir on local sys - needs to be modified to were you keep data
fileNames <- c('en_US.blogs.txt', 'en_US.news.txt', 'en_US.twitter.txt')
fullpaths <- sprintf("%s%s", dataDir, fileNames)
## Different issues with each data file forces use of different read functions
getFileLines <- function(fileId, dataDir=dataDir, fnames=fileNames) {
    if(grep(fileId, fnames) > 0) index <- grep(fileId, fnames)
    else {
        cat('getFileLines could undestand what file to read:', fileId)
        return(NULL)
    }
    fileLines <- read_lines(sprintf("%s%s", dataDir, fnames[index]))
    return(fileLines)
}



```