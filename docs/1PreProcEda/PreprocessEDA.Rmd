---
title: "NLP Predicting Next Word Project - Preprocessing & EDA"
author: "Michael Szczepaniak"
date: "May 2016"
output: html_document
url: http://rpubs.com/mszczepaniak/predictnextduo
---
## Synopsis  
World wide mobile internet usage is projected to continue its rapid growth over the next few years.  According to a [Statista Fact Sheet](http://www.statista.com/statistics/284202/mobile-phone-internet-user-penetration-worldwide/), the percentage of mobile phone users accessing the internet will rise to 63.4% in 2019 up from 48.8% in 2014.  This increased ownship has resulted in more people spending increasing amounts of time on mobile devices for email, social networking, banking and other activities. Because typing on these devices is an awkward and tedious task, smart keyboard applications based on predictive text analytics have emerged to make typing easier.  

The goal of this application is to develop a prototype predictive web application that suggests the next word in a text based message based on what has been typed in by the user. For example, a user may type *I love Italian* and the the application might suggest: *food*, *shoes*, or *opera*.

## Acquiring and Cleaning the Data
### Acquiring and Reading the Data
The data was downloaded from [this link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and stored locally. If this link is no longer available, the data can obtained from [here](https://www.dropbox.com/s/uk7d4gp1c7bgmxc/Coursera-SwiftKey.zip?raw=1). The file was download to a directory called **data** in a local project and unzipped there.  The the unzipped data contained four subdirectories: **de_DE** (German), **en_US** (US english), **fi_FI** (Finnish), and **ru_RU** (Russian). This project focuses on the English corpora residing in the **en_US** folder. This folder contains three files named **en_US.blogs.txt**, **en_US.news.txt**, and **en_US.twitter.txt**.

```{r message=FALSE, warning=FALSE, results='hide'}
# install packages if needed
list.of.packages <- c("dplyr", "readr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages) > 0) install.packages(new.packages)
# load libraries
libs <- c("dplyr", "readr")
lapply(libs, require, character.only=TRUE)  # load libs
options(stringsAsFactors = FALSE)  # strings are what we are operating on...
# set parameters
dataDir <- "../data/en_US/"
filenames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
fullpaths <- sprintf("%s%s", dataDir, filenames)
## Different issues with each data file forces use of different read functions
getFileLines <- function(fileId) {
    fileLines <- NULL
    if(fileId == "blogs") {
        fileLines <- readLines(sprintf("%s%s", dataDir, filenames[1]))
    }
    else if(fileId == "news") {
        fileLines <- read_lines(sprintf("%s%s", dataDir, filenames[2]))
    }
    else if(fileId == "twitter") {
        fileLines <- readLines(sprintf("%s%s", dataDir, filenames[3]))
    }
    else {
        cat("getFilesLines: INVALID fileId parameter!")
    }
    
    return(fileLines)
}