---
title: "Understanding the Katz Back-Off Model"
subtitle: "A Natural Language Model Used to Estimate Probabilities of Word Sequences"
author: "Michael Szczepaniak"
date: "May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# setwd("./docs/2 Katz Back-Off")
```

### Introduction
Before one can implement any kind of mathematical model, machine learning or otherwise, they need a solid understanding of how it works. This article describes how the Katz Back-Off (KBO) model works by way of simple examples keeping the math to a minimum. The ideas presented here are implemented in the git project [**PredictNextDuo**](https://github.com/MichaelSzczepaniak/PredictNextDuo) as a web app deployed on [shinyapps.io](https://michael-szczepaniak.shinyapps.io/predictnextduo/).  

Familiarity with basic concepts such as conditional probabilities, the chain rule of probabilities, Markov assumptions, probability density functions, et. al. are helpful, but not required. I will describe these concepts in an easy to understand manner as they arise throughout this article and/or provide references for those who want to dive deeper into a particular concept.

### What is the Katz Back-Off (KBO) Model?
[Wikipedia provides the following definition](https://en.wikipedia.org/wiki/Katz's_back-off_model):

> Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It accomplishes this estimation by "backing-off" to models with smaller histories under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results. [[1]](https://en.wikipedia.org/wiki/Katz's_back-off_model).

#### Language Models
The term *generative n-gram language model* in the above definition is a loaded term which deserves explaination. Let's start with the term *language model* (LM). In the context of Natural Language Processing (NLP), a language model, which is also referred to as a **grammar**, is a mathematical model which is constructed for the purpose of assigning a probability to either a series of words $w_{i-n}, w_{i-n+1}, ... w_i$ which we can denote as $P(w_{i-n}, w_{i-n+1}, ... w_i)$ or the probability of the last word given the previous words as denoted by $P(w_i | w_{i-n}, w_{i-n+1}, ... w_{i-1})$.  This later definition is what we'll be estimating in order to predict the next word of a phrase.

Why would we want to assign a probability to a word or series of words?  The simple answer is that we want to make word predictions based on the highest probability of what we might actually observe.  

#### Generative N-gram Language Model
A *generative* grammar or LM, refers to the quality of languages related to the rules governing its structure or syntax [[2]](https://en.wikipedia.org/wiki/Generative_grammar). Because of these rules, not all combinations of words form valid sentences and therefore combinations that don't conform to these rules should be assigned a low probability.  For example, the sentence *I red a book* should have lower probability than the sentence *I read a book*.

The term **n-gram** refers to the number of consecutive words utilized in a LM. For example, say we see the terms "*I want to eat Chinese*" 7 times and "*I want to eat Italian*" 3 times in a body of text (aka *corpus*), we might want to match the highest probability 5-gram to complete the 4-gram "*I want to eat*" which in this little example would be *Chinese*.

Dan Jurafky describes the motivations and basic ideas behind probablistic n-gram models in [this video [3]](https://www.youtube.com/watch?v=s3kKlUBa3b0) and how to estimate these probabilities in [this video [4]](https://www.youtube.com/watch?v=o-CvoOkVrnY).

### Assigning Probabilities Using the Maximum Likelihood Estimate
The most intuitive way to assign probabilities to a series of event is to count up the number of occurances of each event type (like seeing a particular word or combination of words) and divide by the total number of occurances.  Assigning probilities in this manner is referred to as the **Maximum Likelihood Estimate** (MLE) because such estimates will be higher than the actual or true probability because unobserved events (e.g. combinations of words) have not been accounted for.  These unobserved events take up some of the probability mass in the true probability density function which is not accounted for in the observed distribution.

### Accounting for Probability Mass of Unseen N-grams: Discounting
In discounting, some of the probability mass is taken from observed n-grams and distributed to unobserved n-grams in order to allow us to estimate probabilities of these unseen n-grams.  In the KBO trigram LM, as will be discussed in more detail later in the article, we select an absolute discount which artificially lowers the counts of observed trigrams and distributes this "stolen" probabilty mass to unobserved trigrams.

The basic concept of discounting is most easily understood by way of example as shown in [this lecture [5]](https://class.coursera.org/nlangp-001/lecture/51) and [this lecture [6]](https://class.coursera.org/nlangp-001/lecture/53) given by Michael Collins in his Coursera class on NLP.

Discounting is a key concept integral to KBO and other advanced LMs like [Good-Turing [7]](https://www.youtube.com/watch?v=XdjCCkFUBKU) and [Kneser-Ney [8]](https://www.youtube.com/watch?v=wtB00EczoCM) as described in the lectures by Dan Jurafasky provided in links [[7]](https://www.youtube.com/watch?v=XdjCCkFUBKU) and [[8]](https://www.youtube.com/watch?v=wtB00EczoCM).

### Using N-gram Tables for Prediction - A Little Math
The theory unlying the use of n-grams probability estimates starts with the [Chain rule for conditional probabilities [9]](https://en.wikipedia.org/wiki/Chain_rule_(probability)) applied to compute joint probabilities which states:

1. $P(w_1, w_2, w_3, ...w_n) = \prod_{i=1}^nP(w_i|w_1, w_2... w_{i-1})$

Jurafsky [[3]](https://www.youtube.com/watch?v=s3kKlUBa3b0) applies equation 1. in the following example to illustrate:

2.
\begin{equation}
P(its\:water\:is\:so\:transparent) = P(its) ×P (water\:|\:its) × P(is\:|\:its\:water) × \\ P(so\:|\:its\:water\:is) × P(transparent\:|\:its\:water\:is\:so)
\end{equation}

#### Simplifying N-gram Probability Estimates: Markov Assumption
The example in equation 2. is the full expansion of the Chain Rule using equation 1.  This can be simplified by assuming that the probability of the left side of equation 1. can be approximated by less than the prior $(n-1)$ terms which can be expressed more formally as:

3. $P(w_1, w_2, w_3, ...w_n) \approx \prod_{i=1}^nP(w_i|w_1, w_{i-k}... w_{i-1})$

Making the above approximation is referred to as applying the Markov Assumption.  From the previous example, this is like saying:

4. $P(the\:|\:its\:water\:is\:so\:transparent\:that) \approx P (the\:|\:that)$ or

5. $P(the\:|\:its\:water\:is\:so\:transparent\:that) \approx P (the\:|\:transparent\:that)$

In equation 3., the probability of the bigram *the* given *that* is used to estimate the probability of *the* given *its water is so transparent that*.  In equation 4., the probability of the trigram *the* given *transparent that* is used to make the same estimate.

### Estimating Probabilities
#### Observed N-grams
Using the bigram MLE to estimate the probability of the last word given the previous word can be written as

6. $P(w_i\:|\:w_{i-1}) = \frac{count(w_{i-1},\:w_i)}{count(w_i)} = \frac{c(w_{i-1},\:w_i)}{c(w_i)}$

where $w_i$ is the last word (the one we are trying to predict), $w_{i-1}$ is the word just prior to $w_i$ and *count* has been abbreviated to *c*.  The trigram MLE can be written as:

7. $P(w_i\:|\:w_{i-1}, \:w_{i-2}) = \frac{c(w_{i-2},\:w_{i-1},\:w_i)}{c(w_{i-1},\:w_i)}$

where $w_{i-2}$ is the word prior to $w_{i-1}$.

As long as we have seen one or more bigrams or trigrams, we can use 6. or 7. to estimate the probability of the last word $w_i$ given the previous one ($w_{i-1}$) or two ($w_{i-2},\:w_{i-1}$).  If we want to estimate probabilities using trigrams, but no trigrams are observed, we could "back-off" and estimate the probability using the bigram.  If no bigrams exist, we could estimate using the unigram probability.  The [*Stupid Back-off* (SBO) model](http://www.aclweb.org/anthology/D07-1090.pdf) essentially does this without consideration of unseen n-grams, but has been reported to perform quite well with large scale n-gram tables.  The SBO is one of the two models implemented in the **PredictNextDuo** project, but is not discussed further in this article.  For further details regarding the SBO, the paper written by the creators of this alogrithm can be found [here.](http://www.aclweb.org/anthology/D07-1090.pdf)

#### Accounting for Unobserved N-grams
As mentioned earlier, the KBO algorithm estimates probabilities of unseen n-grams by redistributing some of the probability mass from observed trigrams to those that are unobserved through discounting. So the natural first step in this process is to determine amount of probability mass that gets taken from the observed n-grams.  The second step will be to determine how that taken probability mass is redistributed.

The total probability mass of the observer bigrams and trigrams can be written as:

8. *bigram probability mass* = $\sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c(w_{i-1},\:w)}{c(w_{i-1})}$

9. *trigram probability mass* = $\sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}$

where the terms $w\:\in\:\mathcal{A}(w_{i-1})$ and $w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})$ refer to all the words *w* in the set $\mathcal{A}$ which are all words that terminate an observed bigram starting with $(w_{i-1})$ or trigram starting with $(w_{i-2},\:w_{i-1})$ respectively.  If we define $\gamma_2$ to be the amount of discount taken from observed bigram counts, $\gamma_2$ the amount of discount taken from observed trigram counts, and $c^*$ to be the new discounted counts for observed bigrams and trigrams, then the discounted MLE would be written as:

10. $q_{BO}(w_i\:|\:w_{i-1}) = \frac{c^*(w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}\:\:\:\:$ for bigrams, where

11. $c^*(w_{i-1},\:w) = c(w_{i-1},\:w) - \gamma_2\:\:\:\:$ and

12. $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1}) = \frac{c^*(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}\:\:\:\:$ for trigrams, where

13. $c^*(w_{i-2},\:w_{i-1},\:w) = c(w_{i-2},\:w_{i-1},\:w) - \gamma_3$

From here it follows that the amount of discounted probability mass taken from observed bigrams and trigrams can then be defined as:

14. $\alpha(w_{i-1}) = 1 - \sum\limits_{w\:\in\:\mathcal{A}(w_{i-1})} \frac{c^*(w_{i-1},\:w)}{c(w_{i-1})}\:\:\:\:$ for bigrams, and

15. $\alpha(w_{i-2},\:w_{i-1}) = 1 - \sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c^*(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}\:\:\:\:$ for trigrams.

At this point, we've finished the first step by defining the amount of probability mass taken from the observed bigrams and trigrams as a function of some discounted amount $\gamma_n$.  Now we need to know how to assign this discounted probability mass to unseen bigrams and trigrams.  Katz proposed that this be done proportionally to the backed-off (n-1)-gram probability parameters.  In other words, an unobserved trigram probability would be estimated in proportion to the bigram tail and unobserved bigram probabilities would be estimated in proportion to the unigram tail.

More formally, if the estimate for this backed-off unigram probability is $q_{BO}(w_i)$ and the backed off bigram probability is $q_{BO}(w_i\:|\:w_{i-1})$, then the amount of discounted probability mass $\alpha(w_{i-1})$ assigned to unknown bigrams $q_{BO}(w_i\:|\:w_{i-1})$ and $\alpha(w_{i-2},\:w_{i-1})$ assigned to unknown trigrams $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ would be:

16. $q_{BO}(w_i\:|\:w_{i-1}) = \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum\limits_{w\:\in\:\mathcal{B}(w_{i-1})}q_{ML}(w)}$

17. $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1}) = \alpha(w_{i-2},\:w_{i-1})\frac{q_{BO}(w_i\:|\:w_{i-1})}{\sum\limits_{w\:\in\:\mathcal{B}(w_{i-2},\:w_{i-1})}q_{BO}(w\:|\:w_{i-1})}$

where the terms $w\:\in\:\mathcal{B}(w_{i-1})$ and $w\:\in\:\mathcal{B}(w_{i-2},\:w_{i-1})$ refer to all the words *w* in the set $\mathcal{B}$ which are all words that terminate an **unobserved** bigram starting with $(w_{i-1})$ or trigram starting with $(w_{i-2},\:w_{i-1})$ respectively.  Notice that the subscripts in the $q$ terms on the right side of equation 16. are **ML**.  This is because at this point, we are using the Maximum Likelihood estimate for the unigrams.

#### Steps in Applying the KBO Trigram Alogrithm

0) Organize the data needed to use the equations defined above:
    i. Create tables of unigram, bigram, and trigram counts.
    ii. Select the values for discounts at the bigram and trigram levels: $\gamma_2$ and $\gamma_3$.
1) Select a bigram that precedes the word you want to predict: $(w_{i-2}, w_{i-1})$
2) Calculate the probabilities for words that complete **observed** trigrams from equation 12.: $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$
3) Calculate the probabilities for words that complete **unobserved** trigrams from equation 17.: $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$
4) Select $w_i$ with the highest $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ as the prediction.

#### Calculating Probabilities for Unobserved N-grams

The steps described in the previous section as straightforward, but step 3. deserves futher explaination.  Each $q_{BO}$ term in equation 17. needs to be calculated using either equation 10. or equation 16. depending on whether the bigram $(w_{i-1}, w_i)$ is observed or unobserved.  If the bigram is observed, equation 10. should be used.  If it is unobserved, equation 16. should be used.

### Example of Applying the Algorithm: The Little Corpus That Could

As implied earlier, a corpus is a body of text from which we build and test LMs.  To illustrate how the mathematical formulation of the KBOT model works, it's helpful to look at a simple corpus that is small enough to see n-gram counts, but large enough to illustrate the impact of unforseen n-grams on the calculations.  The following sample corpus was extended from an example provided by Michael Collins [on page 4 of the week 1 lecture questions](https://d396qusza40orc.cloudfront.net/nlangp/quiz_questions/week1.pdf)
```{r ltc}
ltcorpus <- readLines("little_test_corpus1.txt")
ltcorpus
```
In this corpus, SOS and EOS are tokens used to denote *start of sentence* and *end-of-sentence*. Why this is done will become clear later in the example.

#### Step 0. i. Unigram, Bigram and Trigram counts
This article will use the **quanteda** package written by Ken Benoit and Paul Nulty to construct the n-grams.  My experience with this package is that it performs much faster than **tm** and **RWeka** for these types of tasks, but of course, your mileage may vary.  A nice little getting started guide to this package can be found [here](https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html).

The following code was used used to create to create the unigram, bigram, and trigrams tables shown below the code.

```{r message=FALSE, warning=FALSE}
library(quanteda)

## Returns a named vector of n-grams and their associated frequencies
## extracted from the character vector dat.
##
## ng - Defines the type of n-gram to be extracted: unigram if ng=1,
##      bigram if ng=2, trigram if n=3, etc.
## dat - Character vector from which we want to get n-gram counts.
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - sorts the return vector by the names
## sort.by.freq - sorts the return vector by frequency/count
getNgramFreqs <- function(ng, dat, igfs=NULL, sent.parse=FALSE,
                          sort.by.ngram=TRUE, sort.by.freq=FALSE) {
    if(sent.parse) { dat <- breakOutSentences(dat) }
    # http://stackoverflow.com/questions/36629329/
    # how-do-i-keep-intra-word-periods-in-unigrams-r-quanteda
    if(is.null(igfs)) {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, removePunct = FALSE,
                       what = "fasterword", verbose = FALSE)
    } else {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, ignoredFeatures=igfs,
                       removePunct = FALSE, what = "fasterword", verbose = FALSE)
    }
    rm(dat)
    # quanteda docfreq will get the document frequency of terms in the dfm
    ngram.freq <- docfreq(dat.dfm)
    if(sort.by.freq) { ngram.freq <- sort(ngram.freq, decreasing=TRUE) }
    if(sort.by.ngram) { ngram.freq <- ngram.freq[sort(names(ngram.freq))] }
    rm(dat.dfm)
    
    return(ngram.freq)
}

unigrams <- getNgramFreqs(1, ltcorpus)
bigrams <- getNgramFreqs(2, ltcorpus)
trigrams <- getNgramFreqs(3, ltcorpus)
unigrams; bigrams; trigrams
```


#### Step 0. ii. Selecting bigram and trigram discounts

For this example, we'll use $\gamma_2 = \gamma_3 = 0.5$.  In practice, these values would be obtained by cross-validation.  A great treatment of cross-validation can be found in [Chapter 5 of this book [10]](http://www-bcf.usc.edu/~gareth/ISL/) which are discussed by the authors in these three videos: [[11](https://www.youtube.com/watch?v=_2ij6eaaSl0)], [[12](https://www.youtube.com/watch?v=nZAM5OXrktY)], and [[13](https://www.youtube.com/watch?v=S06JpVoNaA0)].

#### Step 1. Select Bigram Preceding Word to be Predicted

For this example, we'll select the bigram: `sell the`

#### Step 2. Calculate Probabilities of Words Completing Observed Trigrams

The code below finds the observed trigrams starting with specified bigram and calculates their probabilities.  In our simple example, we can look at the table of trigrams above and see that there is only one trigram that starts with `sell the` which is `sell the book`.  Applying equations 12. and 13, we get $q_{BO}(book\:|\:sell,\:the) = (1 - 0.5) / 1 = 0.5$ which is also the result given from the code.

```{r}

```

