---
title: "Predicting Next Word Using Katz Back-Off"
subtitle: "Part 2 - N-grams and Exploratory Data Analysis (EDA)"
author: "Michael Szczepaniak"
date: "August 2016"
output: html_document
url: http://rpubs.com/mszczepaniak/predictkbo2ngeda
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Introduction
In Part 1, we partitioned the corpus data into to training and test sets and performed a number of pre-processing steps in our analysis pipeline that got this data ready to build n-gram tables.  The motivation behind building these tables is that they are needed by our language model to make predictions.  How these tables are used is described in detail in [Predicting Next Word Using Katz Back-Off: Part 3 - Understanding the Katz Back-Off Model](http://rpubs.com/mszczepaniak/predictkbo3model), but in this document, we'll focus on generating the unigram, bigram, and trigram tables and doing some exploratory data analysis (EDA) to get a basic understanding of the data along the way.

## Raw Unigrams
We start with the files listed below as described in [Part 1](http://rpubs.com/mszczepaniak/predictkbo1preproc):

+ [en_US.blogs.train.8posteos.txt](https://www.dropbox.com/s/9dx3oo1w5uf8n1t/en_US.blogs.train.8posteos.txt?dl=1)
+ [en_US.news.train.8posteos.txt](https://www.dropbox.com/s/54cvi36161y6pvk/en_US.news.train.8posteos.txt?dl=1)
+ [en_US.twitter.train.8posteos.txt](https://www.dropbox.com/s/6ayhavfnzs5lmqa/en_US.twitter.train.8posteos.txt?dl=1)

From these three files, unigram frequency tables were created using the **makeRawUnigrams** function. Output files from this function were:

+ [en_US.blogs.train.9rawunig.csv](https://www.dropbox.com/s/pv16pe6buubsz4n/en_US.blogs.train.9rawunig.csv?dl=1)
+ [en_US.news.train.9rawunig.csv](https://www.dropbox.com/s/4u90r188ht2etic/en_US.news.train.9rawunig.csv?dl=1)
+ [en_US.twitter.train.9rawunig.csv](https://www.dropbox.com/s/2sskj3taoq9bkbi/en_US.twitter.train.9rawunig.csv?dl=1)

As is discussed in more detail in the Part 3, the Katz Back-Off model uses the concept of discounting in order to estimate probabilities of unseen n-grams.  This technique essentially removes some of the probability mass from observed n-grams and distributes it to unobserved n-grams.  The removed mass is generated by substracting an assigned amount from the counts of each n-grams.  This assigned amount is know as the **discount**.  If we assign a discount of 1, then we are assuming that each singleton encountered would essentially have the same probability of occurace as an unseen n-gram.  To a feel for this idea, it would be useful to know how many words occur once, twice, and so on.  The chart below shows the "count of counts" on the y-axis and the count on the x-axis for each of the corpus files.

```{r}
library(dplyr)
library(ggplot2)
blogs.raw.unigrams <- "https://www.dropbox.com/s/pv16pe6buubsz4n/en_US.blogs.train.9rawunig.csv?dl=1"
blogs.unigrams.raw <- read.csv(blogs.raw.unigrams)
blogs.unigrams.raw <- arrange(blogs.unigrams.raw, desc(freq))
blog_freqs <- group_by(blogs.unigrams.raw, freq)
blogs_cofc <- summarise(blog_freqs, count_of_count = n())
blogs_cofc <- filter(blogs_cofc, freq <= 500)
p <- ggplot(blogs_cofc, aes(x=freq, y=count_of_count))
p <- p + geom_point() + scale_y_log10() + ggtitle("Blogs Count of Frequency vs. Count")
p
```


## Appendix

```{r eval=FALSE}
loadLibs <- function() {
    libs <- c("dplyr", "readr", "stringr", "dplyr", "quanteda",
              "ggplot2", "data.table")
    lapply(libs, require, character.only=TRUE)  # load libs
    options(stringsAsFactors = FALSE)  # strings are what we are operating on...
}

## Returns a named vector of n-grams and their associated frequencies
## extracted from the character vector dat.
##
## ng - Defines the type of n-gram to be extracted: unigram if ng=1,
##      bigram if ng=2, trigram if n=3, etc.
## dat - Character vector from which we want to get n-gram counts.
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - sorts the return vector by the names
## sort.by.freq - sorts the return vector by frequency/count
getNgramFreqs <- function(ng, dat, igfs=NULL,
                          sort.by.ngram=TRUE, sort.by.freq=FALSE) {
    # http://stackoverflow.com/questions/36629329/
    # how-do-i-keep-intra-word-periods-in-unigrams-r-quanteda
    if(is.null(igfs)) {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, removePunct = FALSE,
                       what = "fasterword", verbose = FALSE)
    } else {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, ignoredFeatures=igfs,
                       removePunct = FALSE, what = "fasterword", verbose = FALSE)
    }
    rm(dat)
    # quanteda docfreq will get the document frequency of terms in the dfm
    ngram.freq <- docfreq(dat.dfm)
    if(sort.by.freq) { ngram.freq <- sort(ngram.freq, decreasing=TRUE) }
    if(sort.by.ngram) { ngram.freq <- ngram.freq[sort(names(ngram.freq))] }
    rm(dat.dfm)
    
    return(ngram.freq)
}

## Returns a 2 column data.table. The first column (ngram) contains the
## unigram (if n=1), the bigram (if n=2), etc.. The second column
## (freq) contains the frequency or count of the ngram found in linesCorpus.
##
## linesCorpus - character vector
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - If TRUE (default), returned table is sorted by ngram
## sort.by.freq - If TRUE, returned table is sorted by frequency, default=FALSE
## prefixFilter - string/character vector: If not NULL, tells the function
##                to return only rows where ngram column starts with prefixFilter.
##                If NULL, returns all the ngram and count rows.
getNgramTables <- function(n, linesCorpus, igfs=NULL, sort.by.ngram=TRUE,
                           sort.by.freq=FALSE, prefixFilter=NULL) {
    cat("start getNgramTables:", as.character(Sys.time()), "\n")
    ngrams <- getNgramFreqs(n, linesCorpus, igfs, sort.by.ngram, sort.by.freq)
    ngrams.dt <- data.table(ngram=names(ngrams), freq=ngrams)
    if(length(grep('^SOS', ngrams.dt$ngram)) > 0) {
        ngrams.dt <- ngrams.dt[-grep('^SOS', ngrams.dt$ngram),]
    }
    if(!is.null(prefixFilter)) {
        regex <- sprintf('%s%s', '^', prefixFilter)
        ngrams.dt <- ngrams.dt[grep(regex, ngrams.dt$ngram),]
    }
    cat("FINISH getNgramTables:", as.character(Sys.time()), "\n")
    return(ngrams.dt)
}

## Creates and writes out the raw unigram frequecy tables for each of the 
## corpus data files.  These are the initial unigram tables that include the
## singletons.
makeRawUnigrams <- function(table.dir=ddir, filePrefix="en_US.",
                            inFilePostfix=".train.8posteos.txt",
                            outFilePostfix=".train.9rawunig.csv",
                            fileTypes=c("blogs", "news", "twitter")) {
    inPaths <- sprintf("%s%s%s%s", table.dir, filePrefix, fileTypes,
                       inFilePostfix)
    outPaths <- sprintf("%s%s%s%s", table.dir, filePrefix, fileTypes,
                        outFilePostfix)
    for(i in 1:length(inPaths)) {
        charvect <- read_lines(inPaths[i])
        unigrams.raw <- getNgramTables(1, charvect)
        write.csv(unigrams.raw, outPaths[i], row.names = FALSE)
    }
}




```


