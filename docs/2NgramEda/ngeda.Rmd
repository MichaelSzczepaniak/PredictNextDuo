---
title: "Predicting Next Word Using Katz Back-Off"
subtitle: "Part 2 - N-grams and Exploratory Data Analysis (EDA)"
author: "Michael Szczepaniak"
date: "August 2016"
output: html_document
url: http://rpubs.com/mszczepaniak/predictkbo2ngeda
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
In Part 1, we partitioned the corpus data into to training and test sets and performed 8 pre-processing steps in our analysis pipeline that got this data ready to build n-gram tables.  The motivation behind building these tables is that they are needed by our language model to make predictions.  How these tables are used is described in detail in [Predicting Next Word Using Katz Back-Off: Part 3 - Understanding the Katz Back-Off Model](http://rpubs.com/mszczepaniak/predictkbo3model), but in this document, we'll focus first on getting a basic understanding of the data by doing some exploratory analysis.  Following the EDA, we'll focus on generating the unigram, bigram, and trigram tables. 

## Raw Unigrams
We start with the files listed below as described in [Part 1](http://rpubs.com/mszczepaniak/predictkbo1preproc):

+ [en_US.blogs.train.8posteos.txt](https://www.dropbox.com/s/9dx3oo1w5uf8n1t/en_US.blogs.train.8posteos.txt?dl=1)
+ [en_US.news.train.8posteos.txt](https://www.dropbox.com/s/54cvi36161y6pvk/en_US.news.train.8posteos.txt?dl=1)
+ [en_US.twitter.train.8posteos.txt](https://www.dropbox.com/s/6ayhavfnzs5lmqa/en_US.twitter.train.8posteos.txt?dl=1)

From these three files, we create unigram frequency tables using the **makeRawUnigramTables** function. Output files from this function were:

+ [](?dl=1)
+ [](?dl=1)
+ [](?dl=1)

## Appendix