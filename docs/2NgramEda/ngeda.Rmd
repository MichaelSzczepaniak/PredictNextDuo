---
title: "Predicting Next Word Using Katz Back-Off"
subtitle: "Part 2 - N-grams and Exploratory Data Analysis (EDA)"
author: "Michael Szczepaniak"
date: "August 2016 (initial release)"
output: html_document
url: http://rpubs.com/mszczepaniak/predictkbo2ngeda
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Introduction
In Part 1, we partitioned the corpus data into to training and test sets and performed a number of pre-processing steps in our analysis pipeline that got this data ready to build n-gram tables.  The motivation behind building these tables is that they are needed by our language model to make predictions.  How these tables are used is described in detail in [Predicting Next Word Using Katz Back-Off: Part 3 - Understanding the Katz Back-Off Model](http://rpubs.com/mszczepaniak/predictkbo3model), but in this document, we'll focus on generating the unigram, bigram, and trigram tables and doing some exploratory data analysis (EDA) to get a basic understanding of the data along the way.

## Descriptive Statistics
It's usually a good idea to begin any analysis involving a large number of elements (e.g. words in this context) with some descriptive statistics.  The following functions were written to run against the original unprocessed corpus files to get the initial file sizes, line counts, vocabulary (count of all words), and word types (count of unique words) respectively. The code and results of these functions are shown below:
```{r cache=TRUE}
library(readr)
fnames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
datDir <- "../../data/en_US/originals/"
## Taken from http://rpubs.com/mszczepaniak/predictkbo1preproc
## Reads the text corpus data file and returns a character array where every
## element is a line from the file.
## fileId = string, text fragment of file name to be read e.g. 'blogs', 'news',
##          or 'twit'
## dataDir = path to data file to be read
## fnames = file names to be read which have fileId fragments
getFileLines <- function(fileId, dataDir=datDir, fileNames=fnames) {
    if(grep(fileId, fnames) > 0) index <- grep(fileId, fnames)
    else {
        cat('getFileLines could undestand what file to read:', fileId)
        return(NULL)
    }
    fileLines <- read_lines(sprintf("%s%s", dataDir, fnames[index]))
    return(fileLines)
}

## Returns the file size in Mb
getFileSize <- function(dataDir=datDir, filename) {
    inputFilePath <- paste0(dataDir, filename)
    return(file.size(inputFilePath) / (2^20))  # convert to MB
}

getLineCount <- function(dataDir=datDir, fileType) {
    return(length(getFileLines(fileType)))
}

## Somewhat crude estimate of word count, but is very close to other methods.
## Assumes that words are separated by spaces.
getWordCount <- function(fileType) {
    f <- getFileLines(fileType)
    return(length(unlist(strsplit(f, " "))))
}

## Returns the number of unique words (tokens) the fileType file.
getTypeCount <- function(fileType) {
    f <- getFileLines(fileType)
    return(length(unique(unlist(strsplit(f, " ")))))
}
```

```{r cache=TRUE, echo=TRUE, results='hide', message=FALSE}
# gather values for table - see rmarkdown source for how table was created
f1=fnames[1]; f2=fnames[2]; f3=fnames[3]
s1=getFileSize(filename=f1); s2=getFileSize(filename=f2); s3=getFileSize(filename=f3)
lc1 <- suppressWarnings(getLineCount(fileType="blogs"))  # about 15 sec's
lc2 <- suppressWarnings(getLineCount(fileType="news"))   # less than 10 sec's
lc3 <- suppressWarnings(getLineCount(fileType="twit"))   # about 15 sec's
voc1=getWordCount("blogs"); voc2=getWordCount("news"); voc3=getWordCount("twitter")
wt1=getTypeCount("blogs"); wt2=getTypeCount("news"); wt3=getTypeCount("twitter")
```
<!--note colon trick to right align text in table -->

File Name | File Size (MB) | Line Count | Word Count (all words) | Word Types (unique words)
----------|---------------:|-----------:|----------:|----------:
`r f1`    | `r round(s1, 2)` | `r lc1` | `r voc1` | `r wt1`
`r f2`    | `r round(s2, 2)` | `r lc2` | `r voc2` | `r wt2`
`r f3`    | `r round(s3, 2)` | `r lc3` | `r voc3` | `r wt3`

## Unigrams
From this point forward, all code for the remainder of this portion of the analysis will be listed in the **Appendix**.  With the raw corpus files in context, we continue the EDA with the files listed below as described in [Part 1](http://rpubs.com/mszczepaniak/predictkbo1preproc):

+ [en_US.blogs.train.8posteos.txt](https://www.dropbox.com/s/9dx3oo1w5uf8n1t/en_US.blogs.train.8posteos.txt?dl=1)
+ [en_US.news.train.8posteos.txt](https://www.dropbox.com/s/54cvi36161y6pvk/en_US.news.train.8posteos.txt?dl=1)
+ [en_US.twitter.train.8posteos.txt](https://www.dropbox.com/s/6ayhavfnzs5lmqa/en_US.twitter.train.8posteos.txt?dl=1)

From these three files, unigram frequency tables were created using the **makeRawUnigrams** function. Output files from this function were:

+ [en_US.blogs.train.9rawunig.csv](https://www.dropbox.com/s/pv16pe6buubsz4n/en_US.blogs.train.9rawunig.csv?dl=1)
+ [en_US.news.train.9rawunig.csv](https://www.dropbox.com/s/4u90r188ht2etic/en_US.news.train.9rawunig.csv?dl=1)
+ [en_US.twitter.train.9rawunig.csv](https://www.dropbox.com/s/2sskj3taoq9bkbi/en_US.twitter.train.9rawunig.csv?dl=1)

As is discussed in more detail in the Part 3, the Katz Back-Off model uses the concept of discounting in order to estimate probabilities of unseen n-grams.  This technique essentially removes some of the probability mass from observed n-grams and distributes it to unobserved n-grams.  The removed mass is generated by substracting an assigned amount from the counts of each n-grams.  This assigned amount is know as the **discount**.  If we assign a discount of 1, then we are assuming that each singleton encountered would essentially have the same probability of occurace as an unseen n-gram.  To get a feel for this idea, it would be useful to know how many words occur once, twice, and so on.  The chart below shows the "count of counts" on the y-axis and the count on the x-axis for each of the corpus files.

```{r cache=TRUE, echo=FALSE, message=FALSE, fig.width=8, fig.height=4}
# https://rstudio.github.io/dygraphs/r-markdown.html
library(dplyr)
library(ggplot2)
# https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html
library(cowplot)

blogs.raw.unigrams <- "https://www.dropbox.com/s/pv16pe6buubsz4n/en_US.blogs.train.9rawunig.csv?dl=1"
blogs.unigrams.raw <- read.csv(blogs.raw.unigrams)
blogs.unigrams.raw <- arrange(blogs.unigrams.raw, desc(freq))
blogs.top10 <- blogs.unigrams.raw[2:11,]  # EOS is most frequent, remove
p1 <- ggplot(blogs.top10, aes(x=reorder(ngram, freq, desc), y=freq))
p1 <- p1 + geom_bar(stat = "identity")
p1 <- p1 + xlab("Unigram") + ylab("Unigram Count")
# See eMPee584 comment under: http://stackoverflow.com/questions/11610377/how-do-i-change-the-formatting-of-numbers-on-an-axis-with-ggplot#comment-47793720
p1 <- p1 + scale_y_continuous(labels=function(n){format(n, scientific = FALSE)})
p1 <- p1 + ggtitle("Top 10 Blogs Unigram Counts")


blog_freqs <- group_by(blogs.unigrams.raw, freq)
blogs_cofc <- summarise(blog_freqs, count_of_count = n())
blogs_cofc <- filter(blogs_cofc, freq <= 500)
p2 <- ggplot(blogs_cofc, aes(x=freq, y=count_of_count))
p2 <- p2 + geom_point()
p2 <- p2 + xlab("Unigram Count") + ylab("Count of Unigram Count")
p2 <- p2 + scale_y_log10(labels=function(n){format(n, scientific = FALSE)})
p2 <- p2 + ggtitle("Blogs Unigram Count of Each Count\nvs. Count")
p2 <- p2 + theme(plot.title = element_text(size=12))
plot_grid(p1, p2, labels = c("1", "2"), ncol = 2, align = 'h')
```

## Bigrams
TODO

## Trigrams
TODO

## Appendix

```{r eval=FALSE}
loadLibs <- function() {
    libs <- c("dplyr", "readr", "stringr", "dplyr", "quanteda",
              "ggplot2", "data.table")
    lapply(libs, require, character.only=TRUE)  # load libs
    options(stringsAsFactors = FALSE)  # strings are what we are operating on...
}

## Returns a named vector of n-grams and their associated frequencies
## extracted from the character vector dat.
##
## ng - Defines the type of n-gram to be extracted: unigram if ng=1,
##      bigram if ng=2, trigram if n=3, etc.
## dat - Character vector from which we want to get n-gram counts.
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - sorts the return vector by the names
## sort.by.freq - sorts the return vector by frequency/count
getNgramFreqs <- function(ng, dat, igfs=NULL,
                          sort.by.ngram=TRUE, sort.by.freq=FALSE) {
    # http://stackoverflow.com/questions/36629329/
    # how-do-i-keep-intra-word-periods-in-unigrams-r-quanteda
    if(is.null(igfs)) {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, removePunct = FALSE,
                       what = "fasterword", verbose = FALSE)
    } else {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, ignoredFeatures=igfs,
                       removePunct = FALSE, what = "fasterword", verbose = FALSE)
    }
    rm(dat)
    # quanteda docfreq will get the document frequency of terms in the dfm
    ngram.freq <- docfreq(dat.dfm)
    if(sort.by.freq) { ngram.freq <- sort(ngram.freq, decreasing=TRUE) }
    if(sort.by.ngram) { ngram.freq <- ngram.freq[sort(names(ngram.freq))] }
    rm(dat.dfm)
    
    return(ngram.freq)
}

## Returns a 2 column data.table. The first column (ngram) contains the
## unigram (if n=1), the bigram (if n=2), etc.. The second column
## (freq) contains the frequency or count of the ngram found in linesCorpus.
##
## linesCorpus - character vector
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - If TRUE (default), returned table is sorted by ngram
## sort.by.freq - If TRUE, returned table is sorted by frequency, default=FALSE
## prefixFilter - string/character vector: If not NULL, tells the function
##                to return only rows where ngram column starts with prefixFilter.
##                If NULL, returns all the ngram and count rows.
getNgramTables <- function(n, linesCorpus, igfs=NULL, sort.by.ngram=TRUE,
                           sort.by.freq=FALSE, prefixFilter=NULL) {
    cat("start getNgramTables:", as.character(Sys.time()), "\n")
    ngrams <- getNgramFreqs(n, linesCorpus, igfs, sort.by.ngram, sort.by.freq)
    ngrams.dt <- data.table(ngram=names(ngrams), freq=ngrams)
    if(length(grep('^SOS', ngrams.dt$ngram)) > 0) {
        ngrams.dt <- ngrams.dt[-grep('^SOS', ngrams.dt$ngram),]
    }
    if(!is.null(prefixFilter)) {
        regex <- sprintf('%s%s', '^', prefixFilter)
        ngrams.dt <- ngrams.dt[grep(regex, ngrams.dt$ngram),]
    }
    cat("FINISH getNgramTables:", as.character(Sys.time()), "\n")
    return(ngrams.dt)
}

## Creates and writes out the raw unigram frequecy tables for each of the 
## corpus data files.  These are the initial unigram tables that include the
## singletons.
makeRawUnigrams <- function(table.dir=ddir, filePrefix="en_US.",
                            inFilePostfix=".train.8posteos.txt",
                            outFilePostfix=".train.9rawunig.csv",
                            fileTypes=c("blogs", "news", "twitter")) {
    inPaths <- sprintf("%s%s%s%s", table.dir, filePrefix, fileTypes,
                       inFilePostfix)
    outPaths <- sprintf("%s%s%s%s", table.dir, filePrefix, fileTypes,
                        outFilePostfix)
    for(i in 1:length(inPaths)) {
        charvect <- read_lines(inPaths[i])
        unigrams.raw <- getNgramTables(1, charvect)
        write.csv(unigrams.raw, outPaths[i], row.names = FALSE)
    }
}

## Code used to generate unigram plots
# https://rstudio.github.io/dygraphs/r-markdown.html
library(dplyr)
library(ggplot2)
# https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html
library(cowplot)

blogs.raw.unigrams <- "https://www.dropbox.com/s/pv16pe6buubsz4n/en_US.blogs.train.9rawunig.csv?dl=1"
blogs.unigrams.raw <- read.csv(blogs.raw.unigrams)
blogs.unigrams.raw <- arrange(blogs.unigrams.raw, desc(freq))
blogs.top10 <- blogs.unigrams.raw[2:11,]  # EOS is most frequent, remove
p1 <- ggplot(blogs.top10, aes(x=reorder(ngram, freq, desc), y=freq))
p1 <- p1 + geom_bar(stat = "identity")
p1 <- p1 + xlab("Unigram") + ylab("Unigram Count")
# See eMPee584 comment under: http://stackoverflow.com/questions/11610377/how-do-i-change-the-formatting-of-numbers-on-an-axis-with-ggplot#comment-47793720
p1 <- p1 + scale_y_continuous(labels=function(n){format(n, scientific = FALSE)})
p1 <- p1 + ggtitle("Top 10 Blogs Unigram Counts")

blog_freqs <- group_by(blogs.unigrams.raw, freq)
blogs_cofc <- summarise(blog_freqs, count_of_count = n())
blogs_cofc <- filter(blogs_cofc, freq <= 500)
p2 <- ggplot(blogs_cofc, aes(x=freq, y=count_of_count))
p2 <- p2 + geom_point()
p2 <- p2 + xlab("Unigram Count") + ylab("Count of Unigram Count")
p2 <- p2 + scale_y_log10(labels=function(n){format(n, scientific = FALSE)})
p2 <- p2 + ggtitle("Blogs Unigram Count of Each Count\nvs. Count")
p2 <- p2 + theme(plot.title = element_text(size=12))
plot_grid(p1, p2, labels = c("1", "2"), ncol = 2, align = 'h')


```


