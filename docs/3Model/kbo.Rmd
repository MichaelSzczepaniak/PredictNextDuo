---
title: "Predicting Next Word Using Katz Back-Off"
author: "Michael Szczepaniak"
date: "August 9, 2016 (last revision)"
output: html_document
subtitle: Part 3 - Understanding the Katz Back-Off Model
url: http://rpubs.com/mszczepaniak/predictkbo3model
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(data.table)
library(readr)
library(stringr)
library(dplyr)
```

### Introduction
Before one can implement any kind of mathematical model, machine learning or otherwise, they need a solid understanding of how it works. This article describes how the Katz Back-Off (KBO) language model works by way of simple examples keeping the math to a minimum. The ideas presented here are implemented in the git project [**PredictNextKBO**](https://github.com/MichaelSzczepaniak/PredictNextKBO) as a web app deployed at (due out in late Aug 2016.) [https://michael-szczepaniak.shinyapps.io/predictnextkbo/](https://michael-szczepaniak.shinyapps.io/predictnextkbo/).  

Familiarity with basic concepts such as conditional probabilities, the chain rule of probabilities, Markov assumptions, probability density functions, et. al. are helpful, but not required. I will describe these concepts in an easy to understand manner as they arise throughout this article and/or provide references for those who want to dive deeper into a particular concept.

### What is the Katz Back-Off (KBO) Model?
[Wikipedia provides the following definition](https://en.wikipedia.org/wiki/Katz's_back-off_model):

> Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It accomplishes this estimation by "backing-off" to models with smaller histories under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results. [[1]](https://en.wikipedia.org/wiki/Katz's_back-off_model).

#### Language Models
The term *generative n-gram language model* in the above definition is a loaded term which deserves explaination. Let's start with the term *language model* (LM). In the context of Natural Language Processing (NLP), a language model, which is also referred to as a **grammar**, is a mathematical model which is constructed for the purpose of assigning a probability to either a series of words $w_{i-n}, w_{i-n+1}, ... w_i$ which we can denote as $P(w_{i-n}, w_{i-n+1}, ... w_i)$ or the probability of the last word given the previous words as denoted by $P(w_i | w_{i-n}, w_{i-n+1}, ... w_{i-1})$.  This later definition is what we'll be estimating in order to predict the next word of a phrase.

Why would we want to assign a probability to a word or series of words?  The simple answer is that we want to make word predictions based on the highest probability of what we might actually observe.  

#### Generative N-gram Language Model
A *generative* grammar or LM, refers to the quality of languages related to the rules governing its structure or syntax [[2]](https://en.wikipedia.org/wiki/Generative_grammar). Because of these rules, not all combinations of words form valid sentences and therefore combinations that don't conform to these rules should be assigned a low probability.  In such a model, we are aware of the rules governing the structure, but this structure is *hidden* in that is it not explicitly determined but rather inferred from model.  For example, the model should assign a lower probability to the sentence *I red a book* than to the sentence *I read a book* based on joint probability distributions of various word groupings known as **n-grams**.

The term **n-gram** refers to the number of consecutive words utilized in a LM. For example, say we see the terms "*I want to eat Chinese*" 7 times and "*I want to eat Italian*" 3 times in a body of text (aka *corpus*), we might want to match the highest probability 5-gram to complete the 4-gram "*I want to eat*" which in this little example would be *Chinese*.

Dan Jurafky describes the motivations and basic ideas behind probablistic n-gram models in [this video [3]](https://www.youtube.com/watch?v=s3kKlUBa3b0) and how to estimate these probabilities in [this video [4]](https://www.youtube.com/watch?v=o-CvoOkVrnY).

### Assigning Probabilities Using the Maximum Likelihood Estimate
The most intuitive way to assign probabilities to a series of event is to count up the number of occurances of each event type (like seeing a particular word or combination of words) and divide by the total number of occurances.  Assigning probilities in this manner is referred to as the **Maximum Likelihood Estimate** (MLE) because such estimates will be higher than the actual or true probability because unobserved events (e.g. combinations of words) have not been accounted for.  These unobserved events take up some of the probability mass in the true probability density function which is not accounted for in the observed distribution.

### Accounting for Probability Mass of Unseen N-grams: Discounting
In discounting, some of the probability mass is taken from observed n-grams and distributed to unobserved n-grams in order to allow us to estimate probabilities of these unseen n-grams.  In the KBO trigram LM, as will be discussed in more detail later in the article, we select an absolute discount which artificially lowers the counts of observed trigrams and distributes this "stolen" probabilty mass to unobserved trigrams.

The basic concept of discounting is most easily understood by way of example as shown in the two lectured given by Michael Collins referenced in the **Appendix 1** at the end of this page.

Discounting is a key concept integral to KBO and other advanced LMs like [Good-Turing [7]](https://www.youtube.com/watch?v=XdjCCkFUBKU) and [Kneser-Ney [8]](https://www.youtube.com/watch?v=wtB00EczoCM) as described in the lectures by Dan Jurafasky provided in links [[7]](https://www.youtube.com/watch?v=XdjCCkFUBKU) and [[8]](https://www.youtube.com/watch?v=wtB00EczoCM).

### Using N-gram Tables for Prediction - A Little Math
The theory unlying the use of n-grams probability estimates starts with the [Chain rule for conditional probabilities [9]](https://en.wikipedia.org/wiki/Chain_rule_(probability)) applied to compute joint probabilities which states:

1. $P(w_1, w_2, w_3, ...w_n) = \prod_{i=1}^nP(w_i|w_1, w_2... w_{i-1})$

Jurafsky [[3]](https://www.youtube.com/watch?v=s3kKlUBa3b0) applies equation 1. in the following example to illustrate:

2. $P(its\:water\:is\:so\:transparent) =  \\ \:\:P(its) \times P (water\:|\:its) \times P(is\:|\:its\:water) \times P(so\:|\:its\:water\:is) \times P(transparent\:|\:its\:water\:is\:so)$

#### Simplifying N-gram Probability Estimates: Markov Assumption
The example in equation 2. is the full expansion of the Chain Rule using equation 1.  This can be simplified by assuming that the probability of the left side of equation 1. can be approximated by less than the prior $(n-1)$ terms which can be expressed more formally as:

3. $P(w_1, w_2, w_3, ...w_n) \approx \prod_{i=1}^nP(w_i|w_1, w_{i-k}... w_{i-1})$

Making the above approximation is referred to as applying the Markov Assumption.  From the previous example, this is like saying:

4. $P(the\:|\:its\:water\:is\:so\:transparent\:that) \approx P (the\:|\:that)$ or

5. $P(the\:|\:its\:water\:is\:so\:transparent\:that) \approx P (the\:|\:transparent\:that)$

In equation 3., the probability of the bigram *the* given *that* is used to estimate the probability of *the* given *its water is so transparent that*.  In equation 4., the probability of the trigram *the* given *transparent that* is used to make the same estimate.

### Estimating Probabilities
#### Observed N-grams
Using the bigram MLE to estimate the probability of the last word given the previous word can be written as

6. $P(w_i\:|\:w_{i-1}) = \frac{count(w_{i-1},\:w_i)}{count(w_i)} = \frac{c(w_{i-1},\:w_i)}{c(w_i)}$

where $w_i$ is the last word (the one we are trying to predict), $w_{i-1}$ is the word just prior to $w_i$ and *count* has been abbreviated to *c*.  The trigram MLE can be written as:

7. $P(w_i\:|\:w_{i-1}, \:w_{i-2}) = \frac{c(w_{i-2},\:w_{i-1},\:w_i)}{c(w_{i-1},\:w_i)}$

where $w_{i-2}$ is the word prior to $w_{i-1}$.

As long as we have seen one or more bigrams or trigrams, we can use 6. or 7. to estimate the probability of the last word $w_i$ given the previous one ($w_{i-1}$) or two ($w_{i-2},\:w_{i-1}$).  If we want to estimate probabilities using trigrams, but no trigrams are observed, we could "back-off" and estimate the probability using the bigram.  If no bigrams exist, we could estimate using the unigram probability.  The [*Stupid Back-off* (SBO) model](http://www.aclweb.org/anthology/D07-1090.pdf) essentially does this without consideration of unseen n-grams, but has been reported to perform quite well with large scale n-gram tables.  The SBO is one of the two models implemented in the **PredictNextDuo** project, but is not discussed further in this article.  For further details regarding the SBO, the paper written by the creators of this alogrithm can be found [here.](http://www.aclweb.org/anthology/D07-1090.pdf)

#### Accounting for Unobserved N-grams
As mentioned earlier, the KBO algorithm estimates probabilities of unseen n-grams by redistributing some of the probability mass from observed trigrams to those that are unobserved through discounting. So the natural first step in this process is to determine amount of probability mass that gets taken from the observed n-grams.  The second step will be to determine how that taken probability mass is redistributed.

The total probability mass of the observer bigrams and trigrams can be written as:

8. *bigram probability mass* = $\sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c(w_{i-1},\:w)}{c(w_{i-1})}$

9. *trigram probability mass* = $\sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}$

where the terms $w\:\in\:\mathcal{A}(w_{i-1})$ and $w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})$ refer to all the words *w* in the set $\mathcal{A}$ which are all words that terminate an observed bigram starting with $(w_{i-1})$ or trigram starting with $(w_{i-2},\:w_{i-1})$ respectively.  If we define $\gamma_2$ to be the amount of discount taken from observed bigram counts, $\gamma_2$ the amount of discount taken from observed trigram counts, and $c^*$ to be the new discounted counts for observed bigrams and trigrams, then the discounted MLE would be written as:

10. $q_{BO}(w_i\:|\:w_{i-1}) = \frac{c^*(w_{i-1},\:w)}{c(w_{i-1})}\:\:\:\:$ for bigrams, where

11. $c^*(w_{i-1},\:w) = c(w_{i-1},\:w) - \gamma_2\:\:\:\:$ and

12. $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1}) = \frac{c^*(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}\:\:\:\:$ for trigrams, where

13. $c^*(w_{i-2},\:w_{i-1},\:w) = c(w_{i-2},\:w_{i-1},\:w) - \gamma_3$

From here it follows that the amount of discounted probability mass taken from observed bigrams and trigrams can then be defined as:

14. $\alpha(w_{i-1}) = 1 - \sum\limits_{w\:\in\:\mathcal{A}(w_{i-1})} \frac{c^*(w_{i-1},\:w)}{c(w_{i-1})}\:\:\:\:$ for bigrams, and

15. $\alpha(w_{i-2},\:w_{i-1}) = 1 - \sum\limits_{w\:\in\:\mathcal{A}(w_{i-2},\:w_{i-1})} \frac{c^*(w_{i-2},\:w_{i-1},\:w)}{c(w_{i-2},\:w_{i-1})}\:\:\:\:$ for trigrams.

At this point, we've finished the first step by defining the amount of probability mass taken from the observed bigrams and trigrams as a function of some discounted amount $\gamma_n$.  Now we need to know how to assign this discounted probability mass to unseen bigrams and trigrams.  Katz proposed that this be done proportionally to the backed-off (n-1)-gram probability parameters.  In other words, an unobserved trigram probability would be estimated in proportion to the bigram tail and unobserved bigram probabilities would be estimated in proportion to the unigram tail.

More formally, if the estimate for this backed-off unigram probability is $q_{ML}(w_i)$ and the backed off bigram probability is $q_{BO}(w_i\:|\:w_{i-1})$, then the amount of discounted probability mass $\alpha(w_{i-1})$ assigned to unknown bigrams $q_{BO}(w_i\:|\:w_{i-1})$ and $\alpha(w_{i-2},\:w_{i-1})$ assigned to unknown trigrams $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ would be:

16. $q_{BO}(w_i\:|\:w_{i-1}) = \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum\limits_{w\:\in\:\mathcal{B}(w_{i-1})}q_{ML}(w)}$

17. $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1}) = \alpha(w_{i-2},\:w_{i-1})\frac{q_{BO}(w_i\:|\:w_{i-1})}{\sum\limits_{w\:\in\:\mathcal{B}(w_{i-2},\:w_{i-1})}q_{BO}(w\:|\:w_{i-1})}$

respectively, where the terms $w\:\in\:\mathcal{B}(w_{i-1})$ and $w\:\in\:\mathcal{B}(w_{i-2},\:w_{i-1})$ refer to all the words *w* in the set $\mathcal{B}$ which are all words that terminate an **unobserved** bigram starting with $(w_{i-1})$ or trigram starting with $(w_{i-2},\:w_{i-1})$ respectively.  Notice that the subscripts in the $q$ terms on the right side of equation 16. are **ML**.  This is because at this point, we are using the Maximum Likelihood estimate for the unigrams.

#### Steps in Applying the KBO Trigram Alogrithm

0) Organize the data needed to use the equations defined above:
    i. Create tables of unigram, bigram, and trigram counts.
    ii. Select the values for discounts at the bigram and trigram levels: $\gamma_2$ and $\gamma_3$.
1) Select a bigram that precedes the word you want to predict: $(w_{i-2}, w_{i-1})$
2) Calculate the probabilities for words that complete **observed** trigrams from equation 12.: $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$
3) Calculate the probabilities for words that complete **unobserved** trigrams from equation 17.: $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$
4) Select $w_i$ with the highest $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ as the prediction.

#### Calculating Probabilities for Unobserved N-grams

The steps described in the previous section are straightforward, but step 3. deserves futher explaination.  Each $q_{BO}$ term in equation 17. needs to be calculated using either equation 10. or equation 16. depending on whether the bigram $(w_{i-1}, w_i)$ is observed or unobserved.  If the bigram is observed, equation 10. should be used.  If it is unobserved, equation 16. should be used.

### Example of Applying the Algorithm: The Little Corpus That Could

As implied earlier, a corpus is a body of text from which we build and test LMs.  To illustrate how the mathematical formulation of the KBO Trigram model works, it's helpful to look at a simple corpus that is small enough to see n-gram counts, but large enough to illustrate the impact of unobserved n-grams on the calculations.  The following sample corpus was extended from an example provided by Michael Collins [on page 4 of the week 1 lecture questions.](https://d396qusza40orc.cloudfront.net/nlangp/quiz_questions/week1.pdf)  This corpus is listed in **Appendix - 2** at the end of this page.
```{r ltc}
ltcorpus <- readLines("little_test_corpus1.txt")
ltcorpus
```
In this corpus, SOS and EOS are tokens used to denote *start of sentence* and *end-of-sentence*.

#### Step 0. i. Unigram, Bigram and Trigram counts
This article will use the **quanteda** package written by Ken Benoit and Paul Nulty to construct the n-grams.  My experience with this package is that it performs much faster than **tm** and **RWeka** for these types of tasks, but of course, your mileage may vary.  A nice little getting started guide to quanteda can be found [here](https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html).

The following code was used to create the unigram, bigram, and trigrams tables shown below the code.

```{r message=FALSE, warning=FALSE}
## Returns a named vector of n-grams and their associated frequencies
## extracted from the character vector dat.
##
## ng - Defines the type of n-gram to be extracted: unigram if ng=1,
##      bigram if ng=2, trigram if n=3, etc.
## dat - Character vector from which we want to get n-gram counts.
## igfs - Character vector of words (features) to ignore from frequency table
## sort.by.ngram - sorts the return vector by the names
## sort.by.freq - sorts the return vector by frequency/count
getNgramFreqs <- function(ng, dat, igfs=NULL,
                          sort.by.ngram=TRUE, sort.by.freq=FALSE) {
    # http://stackoverflow.com/questions/36629329/
    # how-do-i-keep-intra-word-periods-in-unigrams-r-quanteda
    if(is.null(igfs)) {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, removePunct = FALSE,
                       what = "fasterword", verbose = FALSE)
    } else {
        dat.dfm <- dfm(dat, ngrams=ng, toLower = FALSE, ignoredFeatures=igfs,
                       removePunct = FALSE, what = "fasterword", verbose = FALSE)
    }
    rm(dat)
    # quanteda docfreq will get the document frequency of terms in the dfm
    ngram.freq <- docfreq(dat.dfm)
    if(sort.by.freq) { ngram.freq <- sort(ngram.freq, decreasing=TRUE) }
    if(sort.by.ngram) { ngram.freq <- ngram.freq[sort(names(ngram.freq))] }
    rm(dat.dfm)
    
    return(ngram.freq)
}

## Returns a 2 column data.table. The first column: ngram, contains all the
## unigrams, bigrams, or trigrams in the corpus depending on whether
## ng = 1, 2, or 3 respectively. The second column: freq, contains the
## frequency or count of the ngram found in linesCorpus.
##
## ng - Defines the type of n-gram to be extracted: unigram if ng=1,
##      bigram if ng=2, trigram if n=3, etc.
## linesCorpus - character vector which is a line from a corpus file
## prefixFilter - character vector: If not NULL, tells the function to return
##                only rows where the ngram column starts with prefixFilter.
##                If NULL, returns all the ngram and count rows.
getNgramTables <- function(ng, linesCorpus, prefixFilter=NULL) {
    ngrams <- getNgramFreqs(ng, linesCorpus)
    ngrams.dt <- data.table(ngram=names(ngrams), freq=ngrams)
    if(length(grep('^SOS', ngrams.dt$ngram)) > 0) {
        ngrams.dt <- ngrams.dt[-grep('^SOS', ngrams.dt$ngram),]
    }
    if(!is.null(prefixFilter)) {
        regex <- sprintf('%s%s', '^', prefixFilter)
        ngrams.dt <- ngrams.dt[grep(regex, ngrams.dt$ngram),]
    }
    
    return(ngrams.dt)
}

unigs <- getNgramTables(1, ltcorpus)
bigrs <- getNgramTables(2, ltcorpus)
trigs <- getNgramTables(3, ltcorpus)
unigs; bigrs; trigs
```


#### Step 0. ii. Selecting bigram and trigram discounts

For this example, we'll use $\gamma_2 = \gamma_3 = 0.5$.  In practice, these values would be obtained by cross-validation.  A great treatment of cross-validation can be found in [Chapter 5 of this (free) book [10]](http://www-bcf.usc.edu/~gareth/ISL/) which are discussed by the authors in these three videos: [[11](https://www.youtube.com/watch?v=_2ij6eaaSl0)], [[12](https://www.youtube.com/watch?v=nZAM5OXrktY)], and [[13](https://www.youtube.com/watch?v=S06JpVoNaA0)].

#### Step 1. Select Bigram Preceding Word to be Predicted

For this example, we'll select the bigram: `sell the`

#### Step 2. Calculate Probabilities of Words Completing Observed Trigrams

The code below finds the observed trigrams starting with a specified bigram and calculates their probabilities.  In our simple example, we can look at the table of trigrams above and see that there is only one trigram that starts with `sell the` which is `sell the book`.  Applying equations 12. and 13, we get $q_{BO}(book\:|\:sell,\:the) = (1 - 0.5) / 1 = 0.5$ which is also the result provided from the code below.

```{r}

bigPre <- 'sell_the'

## Returns a two column data.table of observed trigrams that start with
## bigramPrefix in the first column named ngram and frequencies/counts in the 
## second column named freq. If no observed trigrams with bigramPrefix exist,
## an empty data.table is returned.
##
## bigramPrefix - single element character vector which is a bigram of the
##                form: word1_word2 e.g. sell_the
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
getObsTrigs <- function(bigramPrefix, trigrams) {
    regex <- sprintf("%s%s", "^", bigramPrefix)
    trigs.winA <- trigrams[grep(regex, trigrams$ngram),]
    return(trigs.winA)
}

## Returns the probability estimate for observed trigrams with bigramPrefix
## calculated from equation 12.
##
## The first column of the datatable are the trigrams corresponding to the
## probability estimate that are in the second column.  If no observed trigrams
## exist, returns NULL.
##
## triDiscount - amount to discount observed trigrams
## bigramPrefix - single-element char array of the form word1_word2
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
calc.qBO.trigramsA <- function(triDiscount=0.5, bigramPrefix, trigrams) {
    obsTrigsA <- getObsTrigs(bigramPrefix, trigrams)
    if(nrow(obsTrigsA) < 1) return(NULL)
    obsCount <- sum(obsTrigsA$freq)
    probs <- (obsTrigsA$freq - triDiscount) / obsCount
    qBO.A <- data.table(ngram=obsTrigsA$ngram, prob=probs)
    return(qBO.A)
}

obsTrigrams <- calc.qBO.trigramsA(bigramPrefix=bigPre, trigrams=trigs)
obsTrigrams
```

#### Step 3. Calculate Probabilities of Words Completing Unobserved Trigrams

This is the the most complex step as it involves backing off to the bigram level.  Here is a breakdown of the sub-steps in these calculations:

i. Find all the words that complete unobserved trigrams. These are the words in the set $w\:\in\:\mathcal{B}(w_{i-2},\:w_{i-1})$ described earlier.
ii. Calculate $\alpha(w_{i-1})$ from equation 14.
iii. Calculate $q_{BO}$ for each bigram in the denominator of equation 17. using equation 10. if the bigram is observed or equation 16. if it is unobserved.
iv. Calculate $\alpha(w_{i-2},\:w_{i-1})$ from equation 15.
v. Calculate $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ for each $w_i$ from equation 17.

##### Step 3. i. Find Unobserved Trigrams

```{r}
## Returns a character vector that are the OBSERVED trigram tail words (OTTW)
## that start with bigramPrefix.
##
## Precondition: bigramPrefix is of the format wi-2_wi-1 where w1-2 is the
## 1st word of an observered trigram and wi-1 is the 2nd/middle word of the
## trigram.
##
## If no trigrams start with bigramPrefix an empty character vector is returned.
##
## bigramPrefix - single-element char array of the form word1_word2
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
getOTTWinA <- function(bigramPrefix, trigrams) {
    regex <- sprintf("%s%s", "^", bigramPrefix)
    trigs.winA <- trigrams[grep(regex, trigrams$ngram), ]
    patToReplace <- sprintf("%s%s", bigramPrefix, "_")
    wInA <- str_replace(trigs.winA$ngram, patToReplace, "")
    return(wInA)
}

## Returns the UNOBSERVED trigram tail words (UTTW) that start with bigramPrefix.
## Precondition: bigramPrefix is of the format wi-2_wi-1 where w1-2 is the
## 1st word of the trigram and wi-1 is the 2nd/middle word of the trigram.
##
## bigramPrefix - single-element char array of the form word1_word2
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
## unigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the unigrams in the corpus. The second column:
##            freq, contains the frequency or count of each unigram.
getUTTWinB <- function(bigramPrefix, trigrams, unigrams) {
    allUnigrams <- unigrams$ngram
    wInA <- getOTTWinA(bigramPrefix, trigrams)
    if(length(wInA) < 1) {
        wInB <- allUnigrams
    } else {
        wInB <- setdiff(allUnigrams, wInA)
    }
    return(wInB)
}

## Returns a vector of unobserved trigrams that start with bigramPrefix
##
## bigramPrefix - first two words of unobserved trigrams we want to estimate
##                probabilities of
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
## unigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the unigrams in the corpus. The second column:
##            freq, contains the frequency or count of each unigram.
getUnobsTrigs <- function(bigramPrefix, trigrams, unigrams) {
    unobsTriTails <- getUTTWinB(bigramPrefix, trigrams, unigrams)
    unobsTrigs <- vector(mode="character", length=length(unobsTriTails))
    for(i in 1:length(unobsTriTails)) {
        unobsTrigs[i] <- sprintf('%s%s%s', bigramPrefix, '_', unobsTriTails[i])
    }
    return(unobsTrigs)
}

unobsTrigrams <- getUnobsTrigs(bigramPrefix=bigPre, trigrams=trigs,
                               unigrams=unigs)
unobsTrigrams
```

##### Step 3. ii. Calculate $\alpha(w_{i-1})$

The code below implements equation 14. to calculate $\alpha(w_{i-1})$

```{r}
## Returns the total probability mass discounted from all observed bigrams.
## This is the amount of probability mass which is redistributed to
## UNOBSERVED bigrams. If no bigrams starting with unigram$ngram[1] exist,
## NULL is returned.
##
## bigDiscount - amount to discount observed bigrams
## bigrams - 2 column data.frame or data.table. The first column: ngram,
##           contains all the bigrams in the corpus. The second column:
##           freq, contains the frequency or count of each bigram.
## unigram - 2 column, single row frequency table The first column: ngram,
##           contains a unigrams. The second column: freq, contains the
##           frequency or count of the unigram.
getAlphaBigram <- function(bigDiscount=0.5, bigrams, unigram) {
    # get all bigrams that start with unigram
    regex <- sprintf("%s%s", "^", unigram$ngram[1])
    bigsThatStartWithUnig <- bigrams[grep(regex, bigrams$ngram),]
    if(nrow(bigsThatStartWithUnig) < 1) return(NULL)
    alphaBi <- 1 - (sum(bigsThatStartWithUnig$freq - bigDiscount) / unigram$freq)
    return(alphaBi)
}


unig <- str_split(bigPre, '_')[[1]][2]
unig <- filter(unigs, ngram == unig)
alphaBig <- getAlphaBigram(bigrams=bigrs, unigram=unig)
alphaBig

```

##### Step 3. iii. Calculate $q_{BO}$ for Bigrams

The code below implements equation 10. to calculate $q_{BO}(w_i\:|\:w_{i-1})$ for observed bigrams and equation 16. for unobserved bigrams:

```{r}

## Returns a 3 column data.table. First column (ngram) = bigrams that are the
## last two words of unobserved trigrams that start with bigramPrefix.
## Second column (btfreq) = frequency/count of the bigram tail of unobserved
##                          trigram
## Third column (utfreq) = frequency/count of the unigram tail word of the
##                         bigram defined in the 2nd column (btfreq)
##
## bigramPrefix - single-element char array of the form word1_word2
## unobsTrigs - character vector of unobserved trigrams of the form:
##              word1_word2_word3, e.g. sell_the_buy
## bigrams - 2 column data.frame or data.table. The first column: ngram,
##           contains all the bigrams in the corpus. The second column:
##           freq, contains the frequency or count of each bigram.
## unigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the unigrams in the corpus. The second column:
##            freq, contains the frequency or count of each unigram.
getUnobsBigramsTable <- function(bigramPrefix, unobsTrigs, bigrams, unigrams) {
    bigramTails <- vector(mode='character', length = length(unobsTrigs))
    bigramTailCounts <- rep(-1, length(unobsTrigs))
    unigramTailCounts <- rep(-1, length(unobsTrigs))
    for(i in 1:length(unobsTrigs)) {
        bigramTail <- str_split(unobsTrigs[i], '_')[[1]]
        unigramTail <- bigramTail[3]
        bigramTail <- sprintf('%s%s%s', bigramTail[2], '_', bigramTail[3])
        bigramTails[i] <- bigramTail
        bigramIndex <- which(bigrams$ngram == bigramTail)
        if(length(bigramIndex) > 0) {
            bigramTailCounts[i] <- bigrams$freq[bigramIndex]
        } else {
            bigramTailCounts[i] <- 0
        }
        unigramTailIndex <- which(unigrams$ngram == unigramTail)
        unigramTailCounts[i] <- unigrams$freq[unigramTailIndex]
    }
    dt <- data.table(ngram=bigramTails, btfreq=bigramTailCounts,
                     utfreq=unigramTailCounts)
    return(dt)
}

## Returns a data.table with the first column (ngram) containing the bigram
## tails of unobserved trigrams that start with bigramPrefix. The second column
## (probs) holds the conditional probability estimate for the last word of the
## bigram tail given the last word of the bigramPrefix (middle word of the 
## unobserved trigram).
##
## bigDiscount - amount to discount observed bigrams
## bigramPrefix - first two words of unobserved trigrams we want to estimate
##                probabilities of
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
## bigrams - 2 column data.frame or data.table. The first column: ngram,
##           contains all the bigrams in the corpus. The second column:
##           freq, contains the frequency or count of each bigram.
## unigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the unigrams in the corpus. The second column:
##            freq, contains the frequency or count of each unigram.
calc.qBO.bigramsB <- function(bigDiscount=0.5, bigramPrefix,
                              trigrams, bigrams, unigrams) {
    unobsTrigs <- getUnobsTrigs(bigramPrefix, trigrams=trigrams,
                                unigrams=unigrams)
    unobBis <- getUnobsBigramsTable(bigramPrefix, unobsTrigs, bigrams, unigrams)
    unobBiProbs <- rep(-1, length(unobBis$ngram))
    # calc discounted prob. mass at bigram level
    unig <- str_split(bigramPrefix, '_')[[1]][2]
    unigram <- filter(unigrams, ngram == unig)
    alphaBig <- getAlphaBigram(bigDiscount, bigrams, unigram)
    uniSumUnobs <- sum(filter(unobBis, btfreq == 0)$utfreq)
    for(i in 1:length(unobBis$ngram)) {
        if(unobBis$btfreq[i] > 0) {
            # bigram tail observed: calc qBO from eqn. 10.
            unobBiProbs[i] <- (unobBis$btfreq[i]-bigDiscount)/unigram$freq[1]
        } else {
            # bigram tail NOT observed: calc qBO w/ bigram from eqn. 16.
            unobBiProbs[i] <- alphaBig * unobBis$utfreq[i] / uniSumUnobs
        }
    }
    dt <- data.table(ngram=unobBis$ngram, prob=unobBiProbs)
    
    return(dt)
}

qboBigs <- calc.qBO.bigramsB(bigDiscount=0.5, bigramPrefix=bigPre,
                             trigrams=trigs, bigrams=bigrs, unigrams=unigs)
qboBigs
```

##### Checking the Bigram Calculations

Before doing the final calculations for the unobserved trigrams, let's do a simple check on our calculations on the bigram level.  In the previous table, all the bigrams except `the_house` are unobserved which means that if we sum all the unobserved bigram probabilities, we should get the total bigram discount which is $\alpha(w_{i-1})$. As we see below, this looks like it checks out.

```{r}
unobs <- qboBigs[-3,]
sum(unobs$prob)
```

##### Step 3. iv. Calculate $\alpha(w_{i-2},\:w_{i-1})$ Discount for Trigrams

The **getAlphaTrigram** function shown below implements equation 15. above to compute the trigram discount.  Here we use it to compute the trigram discount for $q_{BO}(house\:|\:sell,\:the)$:

```{r}
## Returns the total probability mass discounted from all observed trigrams.
## This is the amount of probability mass which is redistributed to
## UNOBSERVED trigrams. If no trigrams starting with bigram$ngram[1] exist,
## NULL is returned.
##
## triDiscount - amount to discount observed trigrams
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
## bigram - single row frequency table where the first col: ngram, is the bigram
##          which are the first two words of unobserved trigrams we want to
##          estimate probabilities of (same as bigramPrefix in previous functions
##          listed above) delimited with '_'. The second column: freq, is the
##          frequency/count of the bigram listed in the 1st column.
getAlphaTrigram <- function(triDiscount=0.5, trigrams, bigram) {
    # get all trigrams that start with bigram
    regex <- sprintf("%s%s", "^", bigram$ngram[1])
    trigsThatStartWithBig <- trigrams[grep(regex, trigrams$ngram),]
    if(nrow(trigsThatStartWithBig) < 1) return(NULL)
    alphaTri <- 1 - (sum(trigsThatStartWithBig$freq - triDiscount) / bigram$freq)
    
    return(alphaTri)
}

bigr <- filter(bigrs, ngram == bigPre)
alphaTrig <- getAlphaTrigram(0.5, trigs, bigr)
alphaTrig
```

##### Step 3. v. Calculate $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$ for Unobserved Trigrams

```{r}
## Returns a two column data table with column names ngram and prob.  The first
## column are trigrams formatted as w1_w2_w3 meaning that the values in the
## prob column are probability estimates of q_BO(w3 | w1, w2) for unobserved
## trigrams that start with bigramPrefix (w1, w2) calculated from equation 17.
##
## bigDiscount - amount to discount observed bigrams
## bigramPrefix - first two words of unobserved trigrams we want to estimate
##                probabilities of
## trigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the trigrams in the corpus. The second column:
##            freq, contains the frequency or count of each trigram.
## bigrams - 2 column data.frame or data.table. The first column: ngram,
##           contains all the bigrams in the corpus. The second column:
##           freq, contains the frequency or count of each bigram.
## unigrams - 2 column data.frame or data.table. The first column: ngram,
##            contains all the unigrams in the corpus. The second column:
##            freq, contains the frequency or count of each unigram.
## alphaTrigr - total amount of discounted probability mass at the trigram level
calc.qBO.trigramB <- function(bigDiscount=0.5, bigramPrefix, trigrams, bigrams,
                              unigrams, alphaTrigr) {
    qBoUnobsBigs <- calc.qBO.bigramsB(bigDiscount=0.5, bigramPrefix=bigramPrefix,
                                      trigrams=trigrams, bigrams=bigrams,
                                      unigrams=unigrams)
    sum.qBoUnobsBigs <- sum(qBoUnobsBigs$prob)
    qBoUnobsTrigProbs <- alphaTrigr * qBoUnobsBigs$prob / sum.qBoUnobsBigs
    qBoUnobsTrigrams <- paste0(str_split(bigramPrefix, '_')[[1]][1],
                               '_', qBoUnobsBigs$ngram)
    dt <- data.table(ngram=qBoUnobsTrigrams, prob=qBoUnobsTrigProbs)
    
    return(dt)
}

qBO.trigs.B <- calc.qBO.trigramB(0.5, bigPre, trigs, bigrs, unigs, alphaTrig)
qBO.trigs.B
```

#### Step 4. Select $w_i$ with the highest $q_{BO}(w_i\:|\:w_{i-2},\:w_{i-1})$

We've done all the calculations required to make our prediction.  These are summarized in the table below:

```{r}
all_trigrams <- rbind(obsTrigrams, qBO.trigs.B)
all_trigrams
sum(all_trigrams$prob)  # good idea to check our work
```
From the above table, we can see that **book** would be our prediction as the code below confirms:
```{r}
predict_trigram <- all_trigrams[which.max(all_trigrams$prob),]
predict_trigram
```

### An Interesting Question

Any good data scientist at this point would be asking themselves some questions about their results, especially if they have not worked with a particular algorithm before.  Let's explore one such question to see if we can deepen our understanding.

In the example we just completed, our prediction of **book** was based on the fact that $q_{BO}(book\:|\:sell,\:the)$ was higher than any other $q_{BO}(w_i\:|\:sell,\:the)$.  But this wasn't really very interesting because *sell the book* was an observed trigram and the next closest probability $q_{BO}(house\:|\:sell,\:the)$ was based on an unobserved trigram *sell the house*.  This leads us to wonder if observed trigrams always trump unobserved trigrams.

We can prove to ourselves that this is not the case with a simple experiment.  Let's redo the above calculations with increased discount rates at both bigram and trigram levels.  If we increase our discount rates from 0.5 to 0.7, what happens?  If we set $\gamma_2 = \gamma_3 = 0.7$, these are the results we get:

```{r}
gamma2=0.7; gamma3=0.7
obsTrigrams <- calc.qBO.trigramsA(gamma3, bigramPrefix=bigPre, trigrams=trigs)
alphaBig <- getAlphaBigram(gamma2, bigrams=bigrs, unigram=unig)
qboBigs <- calc.qBO.bigramsB(bigDiscount=gamma2, bigramPrefix=bigPre,
                             trigrams=trigs, bigrams=bigrs, unigrams=unigs)
alphaTrig <- getAlphaTrigram(gamma3, trigs, bigr)
qBO.trigs.B <- calc.qBO.trigramB(gamma3, bigPre, trigs, bigrs, unigs, alphaTrig)
all_trigrams <- rbind(obsTrigrams, qBO.trigs.B)
all_trigrams
predict_trigram <- all_trigrams[which.max(all_trigrams$prob),]
predict_trigram
```
While changing the discount rate doesn't always change a prediction, this example shows how it can.  As mentioned earlier, the values we should use for these discount rates should be determined by cross-validation and will be strongly dependent upon the corpus used for the training.  Determining the parameters for this model is the focus of [Part 4 of this series (expected release Sep 2016)](http://rpubs.com/mszczepaniak/predictkbo4params).

### Appendix 1 - Michael Collins Coursera Lectures
[Michael Collins of Columbia](https://www.coursera.org/instructor/~312) had an excellent class on NLP which used to be available on Coursera but was removed sometime during the first half of 2016.  At the time of this writing, many of these lectures had been put on youtube and the two referenced on this page could be found at the following links:

[Discounting methods: Part 1 - Katz Bigram](https://www.youtube.com/watch?v=hsHw9F3UuAQ)

[Discounting methods: Part 2 - Katz Trigram]( https://www.youtube.com/watch?v=FedWcgXcp8w)

### Appendix 2 - The little test corpus:

`SOS buy the book EOS`  
`SOS buy the book EOS`  
`SOS buy the book EOS`  
`SOS buy the book EOS`  
`SOS sell the book EOS`  
`SOS buy the house EOS`  
`SOS buy the house EOS`  
`SOS paint the house EOS`  